<!-- ## About me -->
Hi! I am a final year undergraduate student of B.Tech. Computer Engineering at Jamia Millia Islamia University, New Delhi. I'm presently working on my Bachelors Major Thesis on Music Generation from Brain Scans under [Prof. Tanvir Ahmad](https://scholar.google.co.in/citations?user=y7YNRzoAAAAJ&hl=en){:target="_blank" rel="noopener"} where I'm exploring how to leverage Meta's [MusicGen](https://musicgen.com/){:target="_blank" rel="noopener"}, a conditional music LLM, for brain scans.

I'm currently also a part-time researcher at the [HCTL Lab, TU Munich](https://www.edu.sot.tum.de/en/hctl/home/){:target="_blank" rel="noopener"} led by [Prof. Dr. Enkelejda Kasneci](https://scholar.google.com/citations?user=bZVkVvoAAAAJ){:target="_blank" rel="noopener"}. I'm supervised by [Yao Rong](https://yaorong0921.github.io/homepage/){:target="_blank" rel="noopener"} on the topic of self-supervised learning for scanpaths. I was a full-time summer research intern at TUM in 2023  funded by  [DAAD-WISE Scholarship](https://www.daad.in/en/find-funding/scholarship-database/?type=a&q=&status=1&subject=F&onlydaad=1&detail_to_show=0&target=4&origin=4&pg=1&detail_to_show=50015295){:target="_blank" rel="noopener"}. Previously, I've been a summer research intern at IIIT Allahabad under [Prof. Anupam Agarwal](https://scholar.google.co.in/citations?user=mVXjhhgAAAAJ&hl=en){:target="_blank" rel="noopener"} where I worked on ASD diagnosis based on visual attention. I was mentored on my first research [paper](https://www.iosrjournals.org/iosr-jce/papers/Vol25-issue1/Ser-1/E2501013749.pdf){:target="_blank" rel="noopener"} on wild scene text localization in images by [Hitesh Hinduja](https://hitesh-hinduja.mystrikingly.com/){:target="_blank" rel="noopener"}. 

I've had the honor of leading my team sCUDA_Divers to the Grand Finales of [Smart India Hackathon](https://www.sih.gov.in/sih2023){:target="_blank" rel="noopener"} for consecutive years of 2022 and 2023. Our hackathon project on the Super-Resolution of Digital Elevation Models culminated in a research paper published in [IEEE IGARSS 2023](https://ieeexplore.ieee.org/abstract/document/10283196){:target="_blank" rel="noopener"}.

Recently, as part of my Bachelors Minor Thesis, I explored GNNs, LLMs, and RAG techniques for the research competition of Multimodal Emotion-Cause Pair Extraction in Conversations, a [SemEval 2024](https://arxiv.org/abs/2403.04798){:target="_blank" rel="noopener"} Task. (_paper accepted into conference!_)

I'm interested in exploring multimodal LLMs with a specific focus towards the modality of brain scans. Currently exploring [Neural Decoding](https://en.wikipedia.org/wiki/Neural_decoding){:target="_blank" rel="noopener"}. I would love to work on projects on the intersection of neuroscience and artificial intelligence to further our understanding of the brain and intelligence.
 
### Highlights

```19/03/2024``` &nbsp; [Paper on LLMs Accepted in SemEval 2024 Workshop!](https://arxiv.org/abs/2403.04798){:target="_blank" rel="noopener"} \
```06/03/2024``` &nbsp; [2nd Rank in Third Year of Computer Engineering!](./assets/docs/third-year-rank-list.pdf){:target="_blank" rel="noopener"} \
```31/01/2024``` &nbsp; [Team JMI Secured 4th Position at SemEval Task-3!](https://codalab.lisn.upsaclay.fr/competitions/16141#results){:target="_blank" rel="noopener"} \
    ```19/12/2023``` &nbsp; [Grand Finalist of SIH 2023 at KIT, Kolhapur!](./assets/docs/sih2023.pdf){:target="_blank" rel="noopener"} \
```01/08/2023``` &nbsp; [Completed Summer Research Internship at TU Munich!](./assets/docs/TUM.pdf){:target="_blank" rel="noopener"}\
```16/07/2023``` &nbsp; [Master GAN Paper Published in IGARSS 2023!](https://ieeexplore.ieee.org/abstract/document/10283196){:target="_blank" rel="noopener"} \
```10/04/2023``` &nbsp; [3rd Rank in Second Year of Computer Engineering!](./assets/docs/second-year-rank-list.pdf){:target="_blank" rel="noopener"} \
```17/02/2023``` &nbsp; [Text Localization Paper Published in IOSR Journal!](https://www.iosrjournals.org/iosr-jce/papers/Vol25-issue1/Ser-1/E2501013749.pdf){:target="_blank" rel="noopener"} \
```16/01/2023``` &nbsp; [Achieved DAAD-WISE Scholarship!](./assets/docs/DAAD-Scholarship.pdf){:target="_blank" rel="noopener"} \
```25/08/2022``` &nbsp; [Grand Finalist of SIH 2022 at GTU, Ahmedabad!](./assets/docs/sih2022.pdf){:target="_blank" rel="noopener"} \
```15/07/2022``` &nbsp; [Completed Summer Research Internship at IIIT Allahabad!](./assets/docs/IIITA-Cert.pdf){:target="_blank" rel="noopener"}

### Publications

***JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models***  
Arefa, *M. A. Ansari*\*, C. Saxena, T. Ahmad \
SemEval Workshop, 2024 _(ArXiv Preprint)_\
\[[PDF](https://arxiv.org/abs/2403.04798){:target="_blank" rel="noopener"}\] \[[Code](https://github.com/CMOONCS/SemEval-2024_MultiModal_ECPE/tree/main){:target="_blank" rel="noopener"}\]

***Master GAN: Multiple Attention is all you Need: A Multiple Attention Guided Super Resolution Network for Dems***  
A. Mohammed, M. Kashif, M. H. Zama, *M. A. Ansari*\* and S. Ali \
IEEE IGARSS, 2023\
\[[PDF](https://ieeexplore.ieee.org/abstract/document/10283196){:target="_blank" rel="noopener"}\] \[[Code](https://github.com/sheikhazhanmohammed/MASTERGAN){:target="_blank" rel="noopener"}\]

***Revisiting TextFuseNet: Text Context Enhanced Attention Networks For Scene Text Localization***  
H. Hinduja, *M. A. Ansari*\* \
IOSR Journal of Computer Engineering, 2023\
\[[PDF](https://www.iosrjournals.org/iosr-jce/papers/Vol25-issue1/Ser-1/E2501013749.pdf){:target="_blank" rel="noopener"}\] \[[Code](https://github.com/m-abbas-ansari/AttentionText){:target="_blank" rel="noopener"}\]

### Projects
*In order of recency:*

**Music Generation from Brain Scans**  
*Bachelors Major Thesis, 2024*\
\[[Thesis](./assets/docs/Major_Thesis_MusicBrain.pdf){:target="_blank" rel="noopener"}\] 
\[[Slides](https://docs.google.com/presentation/d/1HgKTPPPUA7hyLGmAwSAeAe4Nr9FhS0J9vmu8pdH8lQM/edit?usp=sharing){:target="_blank" rel="noopener"}\]\

Tackled reconstruction of music listened by a subject based on their fMRI scans. Using [Nakai et al's dataset](https://pubmed.ncbi.nlm.nih.gov/34917714/){:target="_blank" rel="noopener"} of 5 subjects' fMRI scans while listening to 540 music pieces. Modified Meta's [MusicGen](https://audiocraft.metademolab.com/musicgen.html){:target="_blank" rel="noopener"} model for music generation conditioned on fMRI scans using the Map Method. Experimented with EnCodec, Chromagram Tokenizer, and T5 encoders, achieving the best performance using the T5 encoder with total averaging (FAD: 8.41, KL: 2.42, MCD: 4.87). Identified the temporal lobe as crucial for music reconstruction, highlighting the importance of auditory processing, language comprehension, and multimodal integration in the neural representation of music.\

**Multimodal Emotion-Cause Analysis in Conversations using in-context learning and instruction-tuned LLMs**  
*SemEval 2024 Workshop Task 3 Competition*\
\[[Paper](https://arxiv.org/abs/2403.04798){:target="_blank" rel="noopener"}\] \[[Code](https://github.com/CMOONCS/SemEval-2024_MultiModal_ECPE/tree/main){:target="_blank" rel="noopener"}\]\
Developed an efficient video captioning technique for conversational videos using GPT-4-Vision. Used Demonstration learning through retrieved examples for emotion recognition and cause prediction using GPT-3.5 for SemEval Task 3. Also implemented instruction-tuned Llama-2 model using QLoRA tecnique. Our approach won rank 4 in the competition. \
_(Paper Accepted!)_\
![Static Badge](https://img.shields.io/badge/--%23412991?logo=openai&label=openai)
![Static Badge](https://img.shields.io/badge/--green?label=%F0%9F%A6%9C%EF%B8%8Flangchain)
![Static Badge](https://img.shields.io/badge/--red?logo=pytorch&label=pytorch)
![Static Badge](https://img.shields.io/badge/--%230467DF?logo=meta&label=llama)
![Static Badge](https://img.shields.io/badge/--%23FFBE00?label=%F0%9F%A4%97accelerate)


**Multimodal Emotion-Cause Pair Extraction using Graph Neural Networks**  
*Bachelors Minor Thesis, 2023*\
\[[Thesis](https://drive.google.com/file/d/1PHWEezwM0vujDF8mSSSPtaIpPMI6GTC5/view?usp=sharing){:target="_blank" rel="noopener"}\] \[[Slides](https://docs.google.com/presentation/d/1aQwf8vZg3c26uXxFzovtNwA92x8kad-IrHTJ6gKgHsk/edit?usp=sharing){:target="_blank" rel="noopener"}\]\
Developed a graph neural network for emotion-cause pair extraction from multimodal conversational data. Utilized CLIP,
BERT, and HTS-AT audio encoder for diverse modality features. Explored multimodal fusion in transformers. Modeled
conversational structure with graph attention networks.\
![Static Badge](https://img.shields.io/badge/--red?logo=pytorch&label=pytorch)
![Static Badge](https://img.shields.io/badge/--%23FFBE00?logo=weightsandbiases&label=%20weights%26biases)
![Static Badge](https://img.shields.io/badge/transformers-5b5d5b?label=%F0%9F%A4%97)

**Real-time Indoor Video Dehazing using Knowledge Distillation**\
*Smart India Hackathon Grand Finale, 2023* \
\[[Slides](https://drive.google.com/file/d/1_YyKU8hJbUSRMd9U_3KBGM3byju15JR9/view?usp=sharing){:target="_blank" rel="noopener"}\] \[[Solution Proposal](https://drive.google.com/file/d/1aWfAYDyxl2WXu0YaAbVPJdc2giFlot6J/view?usp=sharing){:target="_blank" rel="noopener"}\]\
We proposed to modify MAPNet, a UNET-based dehazing network for outdoor environments by replacing some of the blocks with TAM-Net, a 2D convolutional variant for videos. We experimented with distillation by creating a smaller student network for dehazing. During the hackathon, we experimented with Dark Channel Prior and Boundary Contrainst Regularization approaches for benchmarking. \
![Static Badge](https://img.shields.io/badge/--red?logo=pytorch&label=pytorch){:target="_blank" rel="noopener"}
![Static Badge](https://img.shields.io/badge/_-%235C3EE8?logo=opencv&label=opencv){:target="_blank" rel="noopener"}

**Self-Supervised Learning for Free-Viewing Scanpaths**\
*DAAD-WISE Research Project at TU Munich, 2023*\
\[[Slides](https://docs.google.com/presentation/d/17F_fqesKFqedVg6sIdlgmOhkPmw0T2909jU3WfDWfX0/edit?usp=sharing){:target="_blank" rel="noopener"}\] \[[Code](https://github.com/m-abbas-ansari/SSSL){:target="_blank" rel="noopener"}\]\
Eye movements can serve as a proxy for extracting neurological states of a subject. Our goal was to improve classifcation of a subject's cognitive characteristics based on their free-viewing scanpaths on images. We experimented with using a non-contrastive self-supervised learning technique based on BarlowTwins where we implemented novel scanpath distortion techniques to create multiple views of scanpaths. A combined  dataset was created using multiple publicly availble free-viewing scanpath datasets. Experiments demonstrated improvements in the downstream task of Autism detection.\
![Static Badge](https://img.shields.io/badge/--red?logo=pytorch&label=pytorch)
![Static Badge](https://img.shields.io/badge/--%23FFBE00?logo=weightsandbiases&label=%20weights%26biases)



**Super-Resolution of Digital Elevation Models (DEM)**\
*Smart India Hackathon Grand Finale, 2022 & IEEE IGARSS, 2023*\
\[[Slides](https://docs.google.com/presentation/d/13GKR8H8AjNDSijtup7leRnFHKwMQXmlF_HYQ5mBuW0I/edit?usp=sharing){:target="_blank" rel="noopener"}\] \[[Code](https://github.com/m-abbas-ansari/SuperResolution-DEMs){:target="_blank" rel="noopener"}\] \[[Paper](https://ieeexplore.ieee.org/abstract/document/10283196){:target="_blank" rel="noopener"}\]\
Led a team in developing a U-Net based convolutional network with attention for DEM super-resolution in ISROâ€™s Smart India Hackathon. DEMs collected from USGS LiDAR and SRTM, NASA ASTER and ISRO CartoSAT were used to curate the training set. Our team proposed MASTER GAN architecture achieving state-of-the-art results (PSNR 31.024, SSIM 0.908){:target="_blank" rel="noopener"} which got published at IEEE IGARSS 2023 conference.\
![Static Badge](https://img.shields.io/badge/--red?logo=pytorch&label=pytorch)
![Static Badge](https://img.shields.io/badge/_-%235C3EE8?logo=opencv&label=opencv)
![Static Badge](https://img.shields.io/badge/--%23FFBE00?logo=weightsandbiases&label=%20weights%26biases)

**Improved Visual Attention Classification for Autism Spectrum Disorder through Time-Dependent
Representations.**\
*Research Internship Project at IIIT Allahabad, 2022*\
\[[Slides](https://docs.google.com/presentation/d/1yQrqDBjhvNhPT4DhqcKf0RKzAifh-x9uRPPa5BwlsEg/edit?usp=sharing){:target="_blank" rel="noopener"}\] \[[Code](https://github.com/m-abbas-ansari/ASD-Classification){:target="_blank" rel="noopener"}\] \
Trained a deep learning network on Saliency4ASD dataset using ResNet-50 and LSTM using novel time-dependent representations. Encoded embeddings with duration via new techniques such as time-masking and joint embedding. Demonstrated improvements with incorporation of duration as a feature for ASD classification.\
![Static Badge](https://img.shields.io/badge/--red?logo=pytorch&label=pytorch)
![Static Badge](https://img.shields.io/badge/--%23FFBE00?logo=weightsandbiases&label=%20weights%26biases)

**Text Localization using Efficient Attention**\
*Research Project under Mentorship of [Mr Hitesh Hinduja](https://hitesh-hinduja.mystrikingly.com/){:target="_blank" rel="noopener"}*, 2021-23\
\[[Code](https://github.com/m-abbas-ansari/AttentionText){:target="_blank" rel="noopener"}\] \[[Paper](https://www.iosrjournals.org/iosr-jce/papers/Vol25-issue1/Ser-1/E2501013749.pdf){:target="_blank" rel="noopener"}\] \
Modified Mask R-CNN Architecture of Detectron2 library with efficient attention for improved text localization accuracy on SynthText dataset.\
![Static Badge](https://img.shields.io/badge/--red?logo=pytorch&label=pytorch)

**Robust Face Recognition Security System**\
*HackJMI Hackathon Project, 2021*\
\[[Code](https://github.com/m-abbas-ansari/HackJMI2-CheemsGamg){:target="_blank" rel="noopener"}\] \
Developed a robust face recognition security system using MTCNN, VGGFace, and inception-resnet siamese network capable of detecting spoof faces. A website with flask backend was created as MVP which won runner-up position at the hackathon.\
![Static Badge](https://img.shields.io/badge/_-%23FF6F00?logo=tensorflow&label=tensorflow)
![Static Badge](https://img.shields.io/badge/_-%23D00000?logo=keras&label=keras)
![Static Badge](https://img.shields.io/badge/_-%235C3EE8?logo=opencv&label=opencv)
![Static Badge](https://img.shields.io/badge/_-%23000000?logo=flask&label=flask)

**Novel Bible Verse Generator**\
*First Deep Learning Project, 2021*\
\[[Notebook](https://github.com/m-abbas-ansari/Machine-Learning-And-Data-Science/blob/main/pulp_fiction_quote_generation.ipynb){:target="_blank" rel="noopener"}\] \[[Example](https://www.linkedin.com/feed/update/urn:li:activity:6812461393271435264/){:target="_blank" rel="noopener"}\] \
Trained a character-level neural language model on Bible (KJV) using LSTM. Built a loop to generate 1000 characters from the seed text which was the fake Bible verse quote from pulp fiction (1994)\
![Static Badge](https://img.shields.io/badge/_-%23D00000?logo=keras&label=keras)
