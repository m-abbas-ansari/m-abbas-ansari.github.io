<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" id='visual_search'>
            <img src='images/visual-search.jpg' width="160"></div>
        <script type="text/javascript">
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
            Master Research, 2021<br>
          <papertitle>Predicting Visual Attention and Distraction During Visual Search Using Convolutional Neural Networks</papertitle>
        <br>
        Manoosh Samiei, James J. Clark
        <br>
        <a href="https://github.com/ManooshSamiei/Distraction-Visual-Search">GitHub Code</a>
        /
        <a href="https://escholarship.mcgill.ca/concern/theses/5t34sq19q">Thesis</a>
        <br>
        Our dataset analysis report is available on <em>Arxiv</em>: 
        <a href="https://github.com/ManooshSamiei/COCOSearch18_Analysis">Code</a>
        /
        <a href="https://arxiv.org/abs/2209.13771">Paper</a> 
        <br>          
         One publication in <em>Journal of Vision</em> is in progress. <a href="https://arxiv.org/abs/2210.15093#">ArXiv Pre-print</a>   
        <p></p>
        <p>
We present two approaches. Our first method uses a two-stream encoder-decoder network to predict fixation density maps of human observers in visual search. Our second method predicts the segmentation of distractor and target objects during search using a Mask-RCNN segmentation network. We use COCO-Search18 dataset to train/finetune and evaluate our models.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one" id='deepgaze'>
          <img src='images/deepgaze.jpg' width="160">
        </div>
        <script type="text/javascript">
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top">
          <papertitle>Implementing DeepGaze2 Free-viewing Saliency Model, 2020</papertitle>
        <br>
        <a href="https://github.com/ManooshSamiei/DeepGaze2_SaliencyModel">GitHub</a>
        /
        <a href="https://github.com/ManooshSamiei/DeepGaze2_SaliencyModel/blob/main/Report.pdf">Report</a>
        <p></p>
        <p>DeepGaze2 extracts high-level features in images using VGG19 convolutional neural network pretrained for object recognition task. DeepGaze II is trained using a log-likelihood learning framework, and aims to predict where humans look while free-viewing a set of images.</p>
      </td>
    </tr> 
                    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one" id='OD_RL'>

          <img src='images/OD_RL_1.png' width="160">
        </div>
        <script type="text/javascript">

        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top">
          <papertitle>Object Detection with Deep Reinforcement Learning, 2020</papertitle>
        <br>
                      <a href="https://github.com/ManooshSamiei/Object-Detection-Deep-Reinforcement-Learning">GitHub</a> / 
                      <a href="https://arxiv.org/abs/2208.04511">Report</a> / 
                      <a href="https://youtu.be/dcGP9mDnFf0">video</a>			
        <p></p>
        <p>We implmented two papers that formulate object localization as a dynamic Markov decision process based on deep reinforcement learning. We compare two different action settings for this MDP: a hierarchical method and a dynamic method. </p>
      </td>
    </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id='3d_object'>
                <img src='images/3D.png' width="160">
              </div>
              <script type="text/javascript">
              </script>
            </td>
                  <td style="padding:20px;width:75%;vertical-align:top">
                  NeurIPS 2019 Reproducibility Challenge
                  <br>
                  <papertitle>Reproducing CNN2: Viewpoint Generalization via a Binocular Vision, 2019</papertitle>
                <br>
                <a href="https://openreview.net/forum?id=jUJo2RYTOb">Report</a>
                <p></p>
                <p> We replicated the results of the paper “CNN2: Viewpoint Generalization via a Binocular Vision” for two datasets SmallNORB and ModelNet2D.</p>
              </td>
            </tr>
  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id='slf'>
                <img src='images/slf.jpg' width="160">
              </div>
              <script type="text/javascript">
              </script>
            </td>
                  <td style="padding:20px;width:75%;vertical-align:top">
                  <papertitle>Implementation of End-to-End Behavioral Cloning Approach for Lane Following Task in Autonomous Vehicles using Convolutional Neural Networks, 2019</papertitle>
                <br>
                <a href="https://drive.google.com/file/d/11b24VbK3d9jYsrMFm8ikz8AWEWoQKFgA/view?usp=sharing">Thesis in Persian</a>
                <p> </p>
              </td>
            </tr>				
          
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
    <tr>
      <td>
        <heading>Services</heading>
      </td>
    </tr>
  </tbody></table>
  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
              
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/poster.png" width="140"></td>
      <td width="75%" valign="top">
        <br>
        Volunteer in poster sessions in <a href="http://montrealaisymposium.com/"> Montreal AI Symposium 2020</a>
        , and 
        <a href="https://wimlworkshop.org/neurips2020/">WiML 2020</a>
        <br>
        Helped with locating posters and technical issues in gather town platform.
      </td>
    </tr>

    
  </tbody></table>